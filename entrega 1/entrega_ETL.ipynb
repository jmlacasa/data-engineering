{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "import yfinance as yf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONNECT to REDSHIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Redshift successfully!\n"
     ]
    }
   ],
   "source": [
    "with open(r\"C:\\Users\\juanm\\creds\\pwd_redshift.txt\",'r') as f:\n",
    "    pwd= f.read()\n",
    "\n",
    "connection_params = dict(\n",
    "        host='data-engineer-cluster.cyhh5bfevlmn.us-east-1.redshift.amazonaws.com'\n",
    "        , dbname='data-engineer-database'\n",
    "        , user='juanmlacasa_coderhouse'\n",
    "        , password=pwd\n",
    "        , port='5439'\n",
    ")\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        **connection_params\n",
    "    )\n",
    "    print(\"Connected to Redshift successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Unable to connect to Redshift.\")\n",
    "    print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRACT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the parameters for the data to be fetched\n",
    "ticker_info = dict(ticker_symbol = 'AAPL', data_source='Yahoo Finance', source_type = 'Platform', interval='1d', start_date = '2023-01-01', end_date = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "                  Open        High         Low       Close   Adj Close  \\\n",
      "Date                                                                     \n",
      "2023-01-03  130.279999  130.899994  124.169998  125.070000  124.538666   \n",
      "2023-01-04  126.889999  128.660004  125.080002  126.360001  125.823189   \n",
      "2023-01-05  127.129997  127.769997  124.760002  125.019997  124.488876   \n",
      "2023-01-06  126.010002  130.289993  124.889999  129.619995  129.069321   \n",
      "2023-01-09  130.470001  133.410004  129.889999  130.149994  129.597061   \n",
      "\n",
      "               Volume  \n",
      "Date                   \n",
      "2023-01-03  112117500  \n",
      "2023-01-04   89113600  \n",
      "2023-01-05   80962700  \n",
      "2023-01-06   87754700  \n",
      "2023-01-09   70790800  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fetch data for Apple Inc. with a daily interval\n",
    "data = yf.download(ticker_info['ticker_symbol'], interval=ticker_info['interval'], start=ticker_info['start_date'], end=ticker_info['end_date'])\n",
    "\n",
    "# Display the data\n",
    "print(data.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_record(conn, search_value, table_name, entity):\n",
    "    # Check if data source exists\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(f\"\"\"\n",
    "            SELECT {entity}_id FROM {table_name} WHERE {entity}_name = '{search_value}';\n",
    "            \"\"\")\n",
    "        try:\n",
    "            record_id = cur.fetchone()[0]\n",
    "        except TypeError:\n",
    "            record_id = None\n",
    "    if not record_id:\n",
    "        # Insert new data source\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(f\"\"\"\n",
    "                INSERT INTO {table_name} ({entity}_name) VALUES ('{search_value}');\n",
    "            \"\"\")\n",
    "            cur.execute(f\"\"\"\n",
    "                SELECT MAX({entity}_id) FROM {table_name}\n",
    "            \"\"\")\n",
    "            record_id = cur.fetchone()[0]\n",
    "            conn.commit()\n",
    "    return record_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2 3\n"
     ]
    }
   ],
   "source": [
    "asset_id = get_or_create_record(conn, ticker_info['ticker_symbol'], \"assets\", \"asset\")\n",
    "data_source_id = get_or_create_record(conn, ticker_info['data_source'], \"data_sources\", \"source\")\n",
    "source_type_id = get_or_create_record(conn, ticker_info['source_type'], \"source_types\", \"type\")\n",
    "print(asset_id, source_type_id, data_source_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(inplace=True)\n",
    "data.columns = [c.lower().replace(' ', '_') for c in data.columns]\n",
    "data.rename(columns={'date':'timestamp'}, inplace=True)\n",
    "data['asset_id'] = asset_id\n",
    "data['source_id'] = data_source_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psycopg2.extras import execute_values\n",
    "\n",
    "def load_redshift(conn, table_name, dataframe):\n",
    "    with conn.cursor() as curs:\n",
    "        values = [tuple(x) for x in dataframe.to_numpy()]\n",
    "        cols = '\"'+'''\", \"'''.join(data.columns)+'\"'\n",
    "        # define INSERT INTO statement\n",
    "        insert_sql = f\"INSERT INTO {table_name} ({cols}) VALUES %s\"\n",
    "        # Execute the transaction to insert the data\n",
    "        curs.execute(\"BEGIN\")\n",
    "        execute_values(curs, insert_sql, values)\n",
    "        curs.execute(\"COMMIT\")\n",
    "    print('Proceso terminado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso terminado\n"
     ]
    }
   ],
   "source": [
    "load_redshift(conn, 'ohlcv_data', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with conn.cursor() as curs:\n",
    "#     curs.execute(\"ROLLBACK\")\n",
    "#     conn.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coder-de",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a0f6c032de7be79d5c7f13f397ddb255c643e6cfe5a1086054ff4a4b4b03150"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
