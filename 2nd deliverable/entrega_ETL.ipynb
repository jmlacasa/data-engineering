{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_market_calendars as mcal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONNECT to PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Redshift successfully!\n"
     ]
    }
   ],
   "source": [
    "import dotenv\n",
    "import os\n",
    "import psycopg2\n",
    "\n",
    "from os.path import expanduser\n",
    "home = expanduser(\"~\")\n",
    "\n",
    "# dotenv.load_dotenv(fr\"{home}\\creds\\local_postgres.txt\")\n",
    "# schema = 'deliverable_2'\n",
    "dotenv.load_dotenv(fr\"{home}\\creds\\pwd_redshift.txt\")\n",
    "schema = 'juanmlacasa_coderhouse'\n",
    "\n",
    "connection_params = dict(\n",
    "        host=os.getenv('host')\n",
    "        , dbname=os.getenv('dbname')\n",
    "        , user=os.getenv('user')\n",
    "        , password=os.getenv('password')\n",
    "        , port=os.getenv('port')\n",
    ")\n",
    "\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        **connection_params\n",
    "    )\n",
    "    print(\"Connected to Redshift successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Unable to connect to Redshift.\")\n",
    "    print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRACT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the parameters for the data to be fetched\n",
    "ticker_info = dict(ticker_symbol = 'AAPL'\n",
    "                   , data_source='Yahoo Finance'\n",
    "                   , source_type = 'Platform'\n",
    "                   , interval='1d'\n",
    "                   , start_date = '2023-01-15'\n",
    "                   , end_date = '2023-02-15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-01-17</th>\n",
       "      <td>134.830002</td>\n",
       "      <td>137.289993</td>\n",
       "      <td>134.130005</td>\n",
       "      <td>135.940002</td>\n",
       "      <td>135.362488</td>\n",
       "      <td>63646600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-18</th>\n",
       "      <td>136.820007</td>\n",
       "      <td>138.610001</td>\n",
       "      <td>135.029999</td>\n",
       "      <td>135.210007</td>\n",
       "      <td>134.635590</td>\n",
       "      <td>69672800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-19</th>\n",
       "      <td>134.080002</td>\n",
       "      <td>136.250000</td>\n",
       "      <td>133.770004</td>\n",
       "      <td>135.270004</td>\n",
       "      <td>134.695343</td>\n",
       "      <td>58280400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-20</th>\n",
       "      <td>135.279999</td>\n",
       "      <td>138.020004</td>\n",
       "      <td>134.220001</td>\n",
       "      <td>137.869995</td>\n",
       "      <td>137.284286</td>\n",
       "      <td>80223600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-23</th>\n",
       "      <td>138.119995</td>\n",
       "      <td>143.320007</td>\n",
       "      <td>137.899994</td>\n",
       "      <td>141.110001</td>\n",
       "      <td>140.510529</td>\n",
       "      <td>81760300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close   Adj Close  \\\n",
       "Date                                                                     \n",
       "2023-01-17  134.830002  137.289993  134.130005  135.940002  135.362488   \n",
       "2023-01-18  136.820007  138.610001  135.029999  135.210007  134.635590   \n",
       "2023-01-19  134.080002  136.250000  133.770004  135.270004  134.695343   \n",
       "2023-01-20  135.279999  138.020004  134.220001  137.869995  137.284286   \n",
       "2023-01-23  138.119995  143.320007  137.899994  141.110001  140.510529   \n",
       "\n",
       "              Volume  \n",
       "Date                  \n",
       "2023-01-17  63646600  \n",
       "2023-01-18  69672800  \n",
       "2023-01-19  58280400  \n",
       "2023-01-20  80223600  \n",
       "2023-01-23  81760300  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "# Fetch data for Apple Inc. with a daily interval\n",
    "data = yf.download(ticker_info['ticker_symbol']\n",
    "                   , interval=ticker_info['interval']\n",
    "                   , start=ticker_info['start_date']\n",
    "                   , end=ticker_info['end_date'])\n",
    "\n",
    "# Display the data\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSFORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_record(conn, search_value, table_name, entity):\n",
    "    ''''\n",
    "    This function ensures that a record with a specific search value exists in the database.\n",
    "    If the record already exists, it returns its ID.\n",
    "    If the record does not exist, it creates one and then returns the new ID.\n",
    "    '''\n",
    "\n",
    "    # --- Search for Existing Record ---\n",
    "    with conn.cursor() as cur:\n",
    "        # Construct and execute a SELECT query to search for an existing record\n",
    "        cur.execute(f\"SELECT {entity}_id FROM {schema}.{table_name} WHERE {entity}_name = '{search_value}';\")\n",
    "        try:\n",
    "            # If a record is found, retrieve its ID\n",
    "            record_id = cur.fetchone()[0]\n",
    "        except TypeError:\n",
    "            # If no record is found, set record_id to None\n",
    "            record_id = None\n",
    "\n",
    "    # --- Insert Record if Not Found ---\n",
    "    if not record_id:\n",
    "        with conn.cursor() as cur:\n",
    "            # Construct and execute an INSERT query to create a new record\n",
    "            cur.execute(f\"INSERT INTO {schema}.{table_name} ({entity}_name) VALUES ('{search_value}');\")\n",
    "            # Retrieve the ID of the newly created record\n",
    "            cur.execute(f\"SELECT {entity}_id FROM {schema}.{table_name} WHERE {entity}_name = '{search_value}';\")\n",
    "            record_id = cur.fetchone()[0]\n",
    "            # Commit the transaction to save the new record in the database\n",
    "            conn.commit()\n",
    "\n",
    "    # --- Return the Record ID ---\n",
    "    return record_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2 3\n"
     ]
    }
   ],
   "source": [
    "asset_id = get_or_create_record(conn, ticker_info['ticker_symbol'], \"assets\", \"asset\")\n",
    "data_source_id = get_or_create_record(conn, ticker_info['data_source'], \"data_sources\", \"source\")\n",
    "source_type_id = get_or_create_record(conn, ticker_info['source_type'], \"source_types\", \"type\")\n",
    "print(asset_id, source_type_id, data_source_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(inplace=True)\n",
    "data.columns = [c.lower().replace(' ', '_') for c in data.columns]\n",
    "data.rename(columns={'date':'ts'}, inplace=True)\n",
    "data['asset_id'] = asset_id\n",
    "data['source_id'] = data_source_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "from pandas.tseries.offsets import CustomBusinessDay\n",
    "\n",
    "# Define custom business days excluding US federal holidays\n",
    "us_bd = CustomBusinessDay(calendar=USFederalHolidayCalendar())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing dates!\n"
     ]
    }
   ],
   "source": [
    "# import pandas_market_calendars as mcal\n",
    "\n",
    "# Check for missing dates (excluding weekends and holidays)\n",
    "all_days = mcal.date_range(mcal.get_calendar('NYSE').schedule(start_date=ticker_info['start_date'], end_date=ticker_info['end_date'])\n",
    "                , frequency='1D')[:-1]\n",
    "# pd.date_range(start=ticker_info['start_date'], end=ticker_info['end_date'], freq=us_bd)  # B denotes business days\n",
    "missing_days = np.setdiff1d(all_days.date, data['ts'].dt.date)\n",
    "\n",
    "if len(missing_days) > 0:\n",
    "    print(\"Missing dates:\", missing_days)\n",
    "else:\n",
    "    print(\"No missing dates!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for and drop duplicates\n",
    "data.drop_duplicates('ts', inplace=True)\n",
    "data.duplicated('ts').sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psycopg2.extras import execute_values\n",
    "\n",
    "def load_staging_postgresql(conn, table_name, dataframe):\n",
    "    '''\n",
    "    This function loads data from a Pandas DataFrame into a specified table in a PostgreSQL database.\n",
    "    If records with the same primary key exist, the function updates them with the new values.\n",
    "    '''\n",
    "    clean_staging = f'''\n",
    "    DELETE FROM {schema}.staging_{table_name}\n",
    "    '''\n",
    "    # --- Convert DataFrame to List of Tuples ---\n",
    "    # Convert each row of the DataFrame into a tuple and create a list of these tuples\n",
    "    values = [tuple(x) for x in dataframe.to_numpy()]\n",
    "\n",
    "    # --- Format Column Names ---\n",
    "    # Construct a string of column names separated by commas\n",
    "    cols = '\"'+'''\", \"'''.join(dataframe.columns)+'\"'\n",
    "\n",
    "    # --- Prepare SQL Queries ---\n",
    "    # Construct the base INSERT INTO query and append the ON CONFLICT clause\n",
    "    insert_sql = f\"INSERT INTO {schema}.staging_{table_name} ({cols}) VALUES %s\"\n",
    "\n",
    "    # --- Execute Transaction ---\n",
    "    # Execute the query using execute_values for batch insertion\n",
    "    with conn.cursor() as curs:\n",
    "        # Construct and execute an INSERT query to create a new record\n",
    "        curs.execute(clean_staging)\n",
    "        execute_values(curs, insert_sql, values)\n",
    "        conn.commit()\n",
    "\n",
    "    print('Finished loading data into staging PostgreSQL.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_redshift(conn, table_name):\n",
    "\n",
    "    # --- Prepare SQL Queries ---\n",
    "    # Construct DELETE FROM query to delete duplicate records from target table\n",
    "    # Create an INSERT INTO query to insert records from the staging table into the target table\n",
    "\n",
    "\n",
    "    delete_sql = f'''\n",
    "    DELETE FROM {schema}.{table_name}\n",
    "    USING {schema}.staging_{table_name} AS staging\n",
    "    WHERE {schema}.{table_name}.asset_id = staging.asset_id AND\n",
    "        {schema}.{table_name}.source_id = staging.source_id AND\n",
    "        {schema}.{table_name}.ts = staging.ts;\n",
    "    '''\n",
    "    \n",
    "    insert_sql = f'''\n",
    "    INSERT INTO {schema}.{table_name}\n",
    "    SELECT * FROM {schema}.staging_{table_name};\n",
    "    '''\n",
    "\n",
    "    # --- Execute Transaction ---\n",
    "    # Execute the query\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(delete_sql)\n",
    "        cur.execute(insert_sql)\n",
    "        # Commit the transaction to save the new record in the database\n",
    "        conn.commit()\n",
    "\n",
    "    print('Finished loading data into PostgreSQL.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading data into PostgreSQL.\n"
     ]
    }
   ],
   "source": [
    "load_staging_postgresql(conn, 'ohlcv_data', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading data into PostgreSQL.\n"
     ]
    }
   ],
   "source": [
    "merge_redshift(conn, 'ohlcv_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close redshift connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with conn.cursor() as curs:\n",
    "#     curs.execute(\"ROLLBACK\")\n",
    "#     conn.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coder-de",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a0f6c032de7be79d5c7f13f397ddb255c643e6cfe5a1086054ff4a4b4b03150"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
